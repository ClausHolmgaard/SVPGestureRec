{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROGRAM_PATH = os.path.abspath(\"..\")\n",
    "if PROGRAM_PATH not in sys.path:\n",
    "    sys.path.append(PROGRAM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helpers.GeneralHelpers import *\n",
    "from Helpers.RHDHelpers import *\n",
    "from DataHandling.PreProcessing import *\n",
    "from Model.PoolingAndFire import create_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = os.path.expanduser(\"~/results/SVPGestureRec/all_points_test.h5py\")\n",
    "#MODEL_FILE = os.path.expanduser(\"~/results/SqueezeDet/all_points_complete_run_09.h5py\")\n",
    "#MODEL_FILE = os.path.expanduser(\"~/results/SVPGestureRec/all_points_overfit_32.h5py\")\n",
    "RHD_ANNOTATIONS_FILE = os.path.expanduser(\"~/datasets/RHD/RHD_published_v2/training/anno_training.pickle\")\n",
    "VALIDATION_DIR = os.path.expanduser(\"~/datasets/RHD/processed/validation\")\n",
    "TRAIN_DIR = os.path.expanduser(\"~/datasets/RHD/processed/train\")\n",
    "VALIDATION_ANNOTATIONS = os.path.expanduser(\"~/datasets/RHD/processed/validation/annotations/\")\n",
    "TRAIN_ANNOTATIONS = os.path.expanduser(\"~/datasets/RHD/processed/train/annotations/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 413/413 [00:00<00:00, 4368.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating annotations in directory: /home/clh/datasets/RHD/processed/validation\n",
      "Using annotation file: /home/clh/datasets/RHD/RHD_published_v2/training/anno_training.pickle\n",
      "And outputting to: /home/clh/datasets/RHD/processed/validation/annotations/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_rhd_annotations(RHD_ANNOTATIONS_FILE,\n",
    "                       VALIDATION_ANNOTATIONS,\n",
    "                       VALIDATION_DIR,\n",
    "                       fingers='ALL',\n",
    "                       hands_to_annotate='BOTH',\n",
    "                       annotate_non_visible=False,\n",
    "                       force_new_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 16\n",
    "EPSILON = 1e-16\n",
    "\n",
    "LABEL_WEIGHT = 1.0\n",
    "OFFSET_LOSS_WEIGHT = 1.0\n",
    "OFFSET_SCALE = int(320 / 20)\n",
    "\n",
    "NUM_CLASSES = 21\n",
    "NUM_HANDS = 2\n",
    "\n",
    "l = create_loss_function(20,\n",
    "                         20,\n",
    "                         LABEL_WEIGHT,\n",
    "                         OFFSET_SCALE,\n",
    "                         OFFSET_LOSS_WEIGHT,\n",
    "                         NUM_CLASSES,\n",
    "                         EPSILON,\n",
    "                         BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_FILE, custom_objects={'loss_function': l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dir = TRAIN_DIR\n",
    "annotations_dir = TRAIN_ANNOTATIONS\n",
    "NUM_SAMPLES_TO_CHECK = BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_to_check = []\n",
    "#for fi in os.listdir(samples_dir):\n",
    "#    if fi.endswith('png'):\n",
    "#        samples_to_check.append(int(fi.split('.')[0]))\n",
    "#        if len(samples_to_check) >= NUM_SAMPLES_TO_CHECK:\n",
    "#            break\n",
    "\n",
    "#print(samples_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23876 19929 18711 33603 34339  8211 25285  3796 21595 16674 30724 19270\n",
      " 29664 16939  3106  5642]\n"
     ]
    }
   ],
   "source": [
    "samples = np.array(get_all_samples(samples_dir, sample_type='png'))\n",
    "ind = np.random.randint(0, len(samples), size=NUM_SAMPLES_TO_CHECK)\n",
    "samples_to_check = samples[ind]\n",
    "print(samples_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-64901936d4c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                         \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                         \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                         greyscale=True)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mimages_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/SVPGestureRec/DataHandling/PreProcessing.py\u001b[0m in \u001b[0;36mload_data_with_anchors\u001b[0;34m(samples, data_dir, anno_dir, image_width, image_height, anchor_width, anchor_height, offset_scale, sample_type, num_classes, only_images, greyscale, progressbar)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "labels, images = load_data_with_anchors(samples_to_check,\n",
    "                                        samples_dir,\n",
    "                                        annotations_dir,\n",
    "                                        320, 320,\n",
    "                                        20, 20,\n",
    "                                        OFFSET_SCALE,\n",
    "                                        'png',\n",
    "                                        num_classes=NUM_CLASSES,\n",
    "                                        greyscale=True)\n",
    "\n",
    "images_show = np.zeros((len(images), 320, 320, 3), dtype=np.uint8)\n",
    "#for c, im in enumerate(images):\n",
    "#    #images_show[c] = cv2.cvtColor((im * 255.0).astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "#    images_show[c][:, :, 0] = im.reshape(320, 320) * 255.0\n",
    "#    images_show[c][:, :, 1] = im.reshape(320, 320) * 255.0\n",
    "#    images_show[c][:, :, 2] = im.reshape(320, 320) * 255.0\n",
    "\n",
    "for c, s in enumerate(samples_to_check):\n",
    "    images_show[c] = load_image(samples_dir, s)\n",
    "\n",
    "print(np.max(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in labels:\n",
    "    print(l.shape)\n",
    "    for i in range(NUM_CLASSES):\n",
    "        offset_xs = l[:, :, NUM_CLASSES+i*2]\n",
    "        offset_ys = l[:, :, NUM_CLASSES+1+i*2]\n",
    "        \n",
    "        print(f\"Offset max x: {np.max(offset_xs)}\")\n",
    "        print(f\"Offset max y: {np.max(offset_ys)}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_hand = []\n",
    "left_hand = []\n",
    "\n",
    "with open(RHD_ANNOTATIONS_FILE, 'rb') as f:\n",
    "    annotations = pickle.load(f)\n",
    "    \n",
    "for s in samples_to_check:\n",
    "    right_hand.append(get_right_hand(s, annotations))\n",
    "    left_hand.append(get_left_hand(s, annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.zeros(labels.shape)\n",
    "process_times = []\n",
    "for c, im in enumerate(images):\n",
    "    before = time.time()\n",
    "    #res.append(model.predict(im.reshape(1, 320, 320, 1)))\n",
    "    res[c] = model.predict(im.reshape(1, 320, 320, 1))\n",
    "    process_times.append(time.time() - before)\n",
    "    \n",
    "process_times = np.array(process_times)\n",
    "print(f\"Average processing time: {np.mean(process_times)}\")\n",
    "print(f\"Max processing time: {np.max(process_times)}\")\n",
    "print(f\"Min processing time: {np.min(process_times)}\")\n",
    "print(f\"Average processing time excepting first: {np.mean(process_times[1:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, l in enumerate(labels):\n",
    "    for i in range(NUM_CLASSES):\n",
    "        finger_index = i\n",
    "        if finger_index >= 21:\n",
    "            finger_index -= 21\n",
    "        \n",
    "        single_label = np.zeros((20, 20, 3))\n",
    "        single_label[:, :, 0] = labels[c, :, :, i]\n",
    "        single_label[:, :, 1] = labels[c, :, :, NUM_CLASSES+i*2]\n",
    "        single_label[:, :, 2] = labels[c, :, :, NUM_CLASSES+1+i*2]\n",
    "        \n",
    "        anchors = get_anchors(320, 320, 20, 20)\n",
    "\n",
    "        label_points = get_all_points_from_prediction(single_label,\n",
    "                                                      anchors,\n",
    "                                                      threshold=1.0,\n",
    "                                                      offset_weight=OFFSET_SCALE)\n",
    "        \n",
    "        x_points = np.zeros(len(label_points))\n",
    "        y_points = np.zeros(len(label_points))\n",
    "        for counter, p in enumerate(label_points):\n",
    "            #x = p[0] + p[2]\n",
    "            #y = p[1] + p[3]\n",
    "            x_points[counter] = p[0] + p[2]\n",
    "            y_points[counter] = p[1] + p[2]\n",
    "        \n",
    "        if len(label_points) > 0:\n",
    "            x = np.mean(x_points)\n",
    "            y = np.mean(y_points)\n",
    "\n",
    "            print(f\"{FINGER_MAP_INV[finger_index]}: ({x:.2f}, {y:.2f}), anchor point: ({p[0]:.2f}, {p[1]:.2f}), offset: ({p[2]:.2f}, {p[3]:.2f})\")\n",
    "\n",
    "            cv2.circle(images_show[c], (int(x), int(y)), 1, (0, 255, 0), thickness=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.9\n",
    "\n",
    "for c, r in enumerate(res):\n",
    "    #print(r.shape)\n",
    "    #pred = r.reshape(20, 20, 3)\n",
    "    \n",
    "    for i in range(NUM_CLASSES):\n",
    "        \n",
    "        finger_index = i\n",
    "        if finger_index >= 21:\n",
    "            finger_index -= 21\n",
    "        \n",
    "        pred = np.zeros((20, 20, 3))\n",
    "        \n",
    "        pred[:, :, 0] = res[c, :, :, i]\n",
    "        pred[:, :, 1] = res[c, :, :, NUM_CLASSES+i*2]\n",
    "        pred[:, :, 2] = res[c, :, :, NUM_CLASSES+1+i*2]\n",
    "        \n",
    "        #print(np.max(pred[:, :, 0]))\n",
    "        \n",
    "        anchors = get_anchors(320, 320, 20, 20)\n",
    "        max_val = np.max(pred[:,:,0])\n",
    "        #max_val = 0.9\n",
    "        pred_point = get_all_points_from_prediction(pred,\n",
    "                                                    anchors,\n",
    "                                                    threshold=max_val*0.9,\n",
    "                                                    offset_weight=OFFSET_SCALE,\n",
    "                                                    is_label=False)\n",
    "        \n",
    "        # Find a way to handle outliers\n",
    "        x_points = np.zeros(len(pred_point))\n",
    "        y_points = np.zeros(len(pred_point))\n",
    "        for counter, p in enumerate(pred_point):\n",
    "            #x = p[0] + p[2]\n",
    "            #y = p[1] + p[3]\n",
    "            x_points[counter] = p[0] + p[2]\n",
    "            y_points[counter] = p[1] + p[2]\n",
    "        \n",
    "        x = np.mean(x_points)\n",
    "        y = np.mean(y_points)\n",
    "        \n",
    "        print(f\"{FINGER_MAP_INV[finger_index]}  Predicted point: ({x}, {y}), confidence: {max_val}\")\n",
    "\n",
    "        cv2.circle(images_show[c], (int(x), int(y)), 1, (255, 0, 0), thickness=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def confidence_score(label, prediction, num_classes):\n",
    "    \"\"\"\n",
    "    How often does the model tag the correct anchor.\n",
    "    \"\"\"\n",
    "    error = 0\n",
    "    for i in range(num_classes):\n",
    "        l = label[:, :, i]\n",
    "        p = prediction[:, :, i]\n",
    "        \n",
    "        label_x, label_y = np.where(l==1.0)\n",
    "        try:\n",
    "            label_point = np.array([label_x[0], label_y[0]])\n",
    "            print(f\"label_point: {label_point}\")\n",
    "        except:\n",
    "            #print(\"Error\")\n",
    "            continue\n",
    "\n",
    "        prediction_x, prediction_y = np.where(p==np.max(p))\n",
    "        prediction_point = np.array([prediction_x[0], prediction_y[0]])\n",
    "        \n",
    "        print(f\"prediction_point: {prediction_point}\")\n",
    "        \n",
    "        label_error = np.linalg.norm(label_point - prediction_point)\n",
    "        error += label_error\n",
    "        print(label_error)\n",
    "        \n",
    "        print(\"\")\n",
    "    \n",
    "    return error\n",
    "\n",
    "#print(res.shape)\n",
    "confidence_score(labels[0], res[0], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, subs = plt.subplots(len(images_show), 1, figsize=(15, 8*len(samples_to_check)))\n",
    "for c, im in enumerate(images_show):\n",
    "    subs[c].imshow(im)\n",
    "    #subs[c].set_title(f\"Confidence: {np.max(res[c][:,:,:,0])}\")\n",
    "    subs[c].set_title(f\"{samples_to_check[c]}\")\n",
    "\n",
    "#f = plt.figure(figsize=(20, 20))\n",
    "#plt.imshow(images_show[0], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
